# -*- coding: utf-8 -*-
"""Forecasting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A2vwUPoDqUN68eLVrhD2JfppZp3IFjB0
"""

import pandas as pd
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.seasonal import seasonal_decompose 
from statsmodels.tsa.holtwinters import SimpleExpSmoothing   
from statsmodels.tsa.holtwinters import ExponentialSmoothing
import datetime
import sklearn
from sklearn import *
sklearn.__version__
import numpy as np 
#import numpy as np
#import matplotlib.pyplot as plt
#import seaborn as sns
#%matplotlib inline
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.stattools import acf, pacf
from time import time

from google.colab import files
import io 

uploaded = files.upload()
df = pd.read_excel(io.BytesIO(uploaded['Data_Campbell.xlsx']), sheet_name="DataActual")

df=df[df["Ship_Date"]>datetime.datetime(2018,12,31)]

t1 = df.groupby(["Item_ID", "Division"]).agg(Total_Qty_Item = ("Ship_Qty", "sum")).reset_index()
t2 = df.groupby(["Division"]).agg(Total_Qty_Division = ("Ship_Qty", "sum")).reset_index()
t = pd.merge(t1,t2,how='left',on="Division")
t["scaling_factor"] = t["Total_Qty_Item"] / t["Total_Qty_Division"] #Scaling factor as percentage of actual cummulative demand

t = t.sort_values(["Total_Qty_Item", "Division"], ascending=False)\
    .reset_index().drop("index", axis=1)
top_5 = {}
divisions = list(t["Division"].unique())

for div in divisions:
    sku = list(t[t["Division"] == div]["Item_ID"][:5])
    top_5[div] = sku
#Selecting items of our interest
items_top_select = []
for div in divisions:
  items_top_select.extend(top_5[div])

df_items= t[t['Item_ID'].isin(items_top_select)] 
df_items=df_items[["Item_ID","Division","scaling_factor"]]

from google.colab import files
import io 

uploaded = files.upload()
df_division = pd.read_csv(io.BytesIO(uploaded['Division.csv']), parse_dates=['Ship_Date'])

#Holt Winter Forecasting - Returns training and testing datasets with predicted values 
def Holt_Winter_Forecasting(training_dataset,testing_dataset,operation):
  training_dataset.head(5)
  training_dataset=training_dataset.set_index("Ship_Date")
  testing_dataset=testing_dataset.set_index("Ship_Date")
  y_train=training_dataset["Ship_Qty"]
  fitted_model = ExponentialSmoothing(y_train,trend=operation,seasonal=operation,seasonal_periods=13).fit(optimized=True)
  testing_dataset["Prediction"] = list(fitted_model.forecast(len(testing_dataset)))
  
  training_dataset["Prediction"]=list(fitted_model.forecast(len(training_dataset)))
  return training_dataset.reset_index(),testing_dataset.reset_index()

#RMSE AND MAPE
def calculate_rmse(actual,predicted):
  rmse = np.sqrt(mean_squared_error(actual, predicted)).round(2)
  return rmse
def calculate_mape(actual,predicted):
  mape= np.round(np.mean(np.abs(actual-predicted)/actual)*100,2)
  return mape

#Forecasting Plots
def create_plots(y_train,y_test,plot_title,testing_dataset):
  y_train.plot(legend=True,label='TRAIN')
  y_test.plot(legend=True,label='TEST',figsize=(6,4))
  testing_dataset["Prediction"].plot(legend=True,label='PREDICTION')
  plt.title(plot_title)

"""###***Holt-Winter***

"""

#Train-Test DataSet: 80% training 20% testing
training_division_dataset=df_division[df_division["Ship_Date"]<datetime.datetime(2019,10,1)]
testing_division_dataset=df_division[df_division["Ship_Date"]>=datetime.datetime(2019,10,1)]

#x_train= training_dataset[["Division","Ship_Date"]]
training_division_dataset=training_division_dataset.set_index("Ship_Date")
testing_division_dataset=testing_division_dataset.set_index("Ship_Date")
y_train=training_division_dataset["Ship_Qty"]

#x_test= testing_dataset[["Division","Ship_Date"]]
y_test=testing_division_dataset["Ship_Qty"]

#Forecasting
operation="add"
df_test=pd.DataFrame()
#df_holt=pd.DataFrame(columns=["Ship_Date","Division","Ship_Qty","Prediction"])
for div in divisions:
  training_dataset=training_division_dataset[training_division_dataset["Division"]==div].reset_index()
  testing_dataset=testing_division_dataset[testing_division_dataset["Division"]==div].reset_index()
  training_dataset,testing_dataset=Holt_Winter_Forecasting(training_dataset,testing_dataset,operation)
  df_test=df_test.append(testing_dataset)

#Plotting
df_test=df_test.set_index("Ship_Date")
plot_title="Holt_Winter_Forecast" + "_" + operation
create_plots(y_train,y_test,plot_title,df_test)    
#Merging
df_test=df_test.reset_index()
df_holt = pd.merge(df_test,df_items,how='left',on="Division")
df_holt["Item_forecast"]=df_holt["Prediction"]*df_holt["scaling_factor"]
df_holt=df_holt[["Ship_Date","Division","Item_ID","Item_forecast"]]
df_item_qty=df.groupby(["Item_ID","Division","Ship_Date"]).agg(Item_Ship_Qty = ("Ship_Qty", "sum")).reset_index()
df_holt=pd.merge(df_holt,df_item_qty[["Item_ID","Ship_Date","Item_Ship_Qty"]],on=["Item_ID","Ship_Date"],how="left")
df_holt=df_holt[df_holt["Item_Ship_Qty"]>0]

from google.colab import files

training_division_dataset.to_csv('output2.csv', encoding = 'utf-8-sig') 
files.download('output2.csv')

actual=df_holt["Item_Ship_Qty"]
predicted=df_holt["Item_forecast"]
mape_holt= calculate_mape(actual,predicted)
rmse_holt=calculate_rmse(actual,predicted)
result=pd.DataFrame()
holt_result=pd.DataFrame({'Method':["Holt Winter"],'RMSE':[rmse_holt],'MAPE':[mape_holt]})
result=pd.concat([result,holt_result],axis=0)

result

"""###***SARIMA***

"""

training_division_dataset[training_division_dataset["Division"]=="CA-FS"]["Ship_Qty"].plot()

training_division_dataset[training_division_dataset["Division"]=="US-RTL"]["Ship_Qty"].plot()

training_division_dataset[training_division_dataset["Division"]=="US-FS"]["Ship_Qty"].plot()

training_division_dataset["Ship_Qty"].plot()

#Division- CA-FS
#Removing Trend
first_diff=df_division[df_division["Division"]=="CA-FS"]["Ship_Qty"].diff()[1:]
training_division_dataset[training_division_dataset["Division"]=="CA-FS"]["Ship_Qty"].diff()[1:].plot()

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
acf_vals = acf(first_diff)
num_lags = 35
plot_acf(first_diff,lags=30)

plot_pacf(first_diff,lags=30)

my_order = (0,1,0)
my_seasonal_order = (1, 1, 1, 2)
# define model
model = SARIMAX(training_division_dataset["Ship_Qty"], order=my_order, seasonal_order=my_seasonal_order)

#fit the model
start = time()
model_fit = model.fit()
end = time()
print('Model Fitting Time:', end - start)

#summary of the model
print(model_fit.summary())

from statsmodels.tsa.stattools import adfuller
adf, pvalue, usedlag_, nobs_, critical_values_, icbest_ = adfuller(first_diff)

training_division_dataset["Ship_Qty"]

from pylab import rcParams
import statsmodels.api as sm
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf



rcParams['figure.figsize'] = 12, 8
decomposition = seasonal_decompose(df_division[df_division["Division"]=="CA-FS"]["Ship_Qty"].sort_index(), model='additive',freq=10)# additive seasonal index
fig = decomposition.plot()
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import itertools
# #set parameter range
# p = range(0,4)
# 
# q= range(0,4)
# 
# d = [0, 1, 2]
# s = [0,10,26]
# # list of all parameter combos
# pdq = list(itertools.product(p, d, q))
# seasonal_pdq = list(itertools.product(p, d, q, s))
# df1=pd.DataFrame(columns=["AIC","Attributes"])
# i=0
# # SARIMA model pipeline
# for param in pdq:
#     for param_seasonal in seasonal_pdq:
#         try:
#             mod = sm.tsa.statespace.SARIMAX(df_division[df_division["Division"]=="CA-FS"]["Ship_Qty"],
#                                     order=param,
#                                     seasonal_order=param_seasonal)
#             results = mod.fit(max_iter = 50, method = 'powell')
#             df1.loc[i,"AIC"]=results.aic
#             add_list=list(param + param_seasonal)
#             df1.loc[i,"Attributes"]=add_list
#         except:
#             continue
#     i=i+1

df1

add_list

print(list(param).extend(list(param_seasonal)))

param

param_seasonal

a=list(param).extend(list(param_seasonal))

a

b=param+param_seasonal

b

c=list(b)

c

df1["AIC"].min()

df1[df1["AIC"]==650.7363361851823]

from google.colab import files
df1.to_csv('CA-FS.csv', encoding = 'utf-8-sig') 
files.download('CA-FS.csv')

df1

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import itertools
# #set parameter range
# p = range(0,4)
# 
# q= range(0,4)
# 
# d = [0, 1, 2]
# s = [0,10,26]
# # list of all parameter combos
# pdq = list(itertools.product(p, d, q))
# seasonal_pdq = list(itertools.product(p, d, q, s))
# df1=pd.DataFrame(columns=["AIC","Attributes"])
# i=0
# # SARIMA model pipeline
# for param in pdq:
#     for param_seasonal in seasonal_pdq:
#         try:
#             mod = sm.tsa.statespace.SARIMAX(df_division[df_division["Division"]=="US-RTL"]["Ship_Qty"],
#                                     order=param,
#                                     seasonal_order=param_seasonal)
#             results = mod.fit(max_iter = 50, method = 'powell')
#             df1.loc[i,"AIC"]=results.aic
#             add_list=list(param + param_seasonal)
#             df1.loc[i,"Attributes"]=add_list
#         except:
#             continue
#     i=i+1

df1["AIC"].min()

df1[df1["AIC"]==849.7069627753621]

from google.colab import files
df1.to_csv('US-RTL.csv', encoding = 'utf-8-sig') 
files.download('US-RTL.csv')

divisions

# Commented out IPython magic to ensure Python compatibility.
# %%time
# def Grid_Search(division)
#   import itertools
#   #set parameter range
#   p = range(0,4)
# 
#   q= range(0,4)
# 
#   d = [0, 1, 2]
#   s = range(0,27)
#   # list of all parameter combos
#   pdq = list(itertools.product(p, d, q))
#   seasonal_pdq = list(itertools.product(p, d, q, s))
#   df1=pd.DataFrame(columns=["AIC","Attributes"])
#   i=0
#   # SARIMA model pipeline
#   for param in pdq:
#       for param_seasonal in seasonal_pdq:
#           try:
#               mod = sm.tsa.statespace.SARIMAX(df_division[df_division["Division"]==division]["Ship_Qty"],
#                                       order=param,
#                                       seasonal_order=param_seasonal)
#               results = mod.fit(max_iter = 50, method = 'powell')
#               df1.loc[i,"AIC"]=results.aic
#               add_list=list(param + param_seasonal)
#               df1.loc[i,"Attributes"]=add_list
#           except:
#               continue
#       i=i+1
#   return df1[df1["AIC"]==df1["AIC"].min()]["Attributes"]

df_division[df_division["Division"]=="US-FS"]

df1

